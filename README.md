# ğŸ§  Temporal Modelling and Classification of Hand Flapping Form in Autism Spectrum Disorder

This repository implements a **pose-based deep learning framework** to classify **hand-flapping forms** in children with Autism Spectrum Disorder (ASD).  
It extends the earlier frequency-based pipeline by **Mohammed (2023)** to include **temporal modelling** and **form classification**, using pose-estimated joint angles and hybrid deep-learning architectures.

---

## ğŸ“– Overview

Hand flapping is a common repetitive behaviour in ASD.  
While earlier methods quantified only **flapping frequency**, this study introduces a system to classify **flapping form**â€”including **symmetry**, **amplitude**, and **hand dominance**â€”using upper-limb kinematic features extracted from video.

### ğŸ” Eight-Class Form Definition

| Class | Description |
|-------|--------------|
| 0 | No flap |
| 1 | Left-only (low amplitude) |
| 2 | Left-only (high amplitude) |
| 3 | Right-only (low amplitude) |
| 4 | Right-only (high amplitude) |
| 5 | Both symmetric (low amplitude) |
| 6 | Both symmetric (high amplitude) |
| 7 | Both asymmetric |

---

## ğŸ§© Pipeline Overview


<img width="1242" height="381" alt="dc2c8d3fa5165222411e7f03f4a9ecc2" src="https://github.com/user-attachments/assets/eced88be-a0b1-4c0a-855f-5d3ca0ef5eba" />


---

## âš™ï¸ Methodology

### 1ï¸âƒ£ Pose Extraction and Feature Computation
- Extract 33 landmarks using **MediaPipe Pose**
- Compute 8 joint angles (shoulderâ€“elbowâ€“wristâ€“hip)
- Derive first- and second-order temporal differences (Î”, Î”Â²)
- Final input tensor: `64 Ã— 24` features per sequence

### 2ï¸âƒ£ Models Implemented
| Model | Type | Notes |
|--------|------|-------|
| Random Forest | ML baseline | Best static accuracy |
| SVM | ML baseline | Sensitive to scaling |
| CNN1D | DL | Captures local temporal patterns |
| Bi-LSTM | DL | Captures long-range dependencies |
| **Hybrid CNNâ€“LSTM** | DL | Combines both spatial and temporal dynamics |

### 3ï¸âƒ£ Training Setup
| Parameter | Value |
|------------|--------|
| Loss | Categorical Cross-Entropy |
| Optimiser | Adam |
| Epochs | 50 |
| Batch Size | 32 |
| Cross-validation | Group K-Fold (per video) |

---

## ğŸ“Š Experimental Results

| Model | Accuracy | Macro-F1 | Observation |
|--------|-----------|-----------|-------------|
| Random Forest | **0.984 Â± 0.013** | **0.943 Â± 0.045** | Highest static accuracy |
| CNN1D | 0.842 Â± 0.052 | 0.581 Â± 0.041 | Learns local motion patterns |
| Bi-LSTM | 0.813 Â± 0.041 | 0.569 Â± 0.033 | Learns long-term dynamics |
| **Hybrid CNNâ€“LSTM** | **0.855 Â± 0.030** | **0.603 Â± 0.026** | Best generalisation and interpretability |

> The hybrid CNNâ€“LSTM demonstrated the most balanced performance among deep models, integrating both spatial and temporal representations.

---

## ğŸ§® Repository Structure

```
src/
 â”œâ”€â”€ preprocessing/       # Pose extraction, angle computation
 â”œâ”€â”€ training/            # ML and DL training scripts
 â”œâ”€â”€ inference/           # Predict and visualise results
 â”œâ”€â”€ analysis/            # Quantitative & qualitative evaluation
 â””â”€â”€ utils/               # Helper functions and configs
configs/
 â””â”€â”€ train_hybrid_len64.json
figures/
 â”œâ”€â”€ pipeline.png
 â””â”€â”€ Figure_4_4_alignment.png
requirements.txt
README.md
.gitignore
LICENSE
weak_labels.csv (optional)
```

---

## ğŸ’» Example Usage

### â–¶ï¸ Preprocess video
```bash
python src/preprocessing/extract_pose.py \
  --input data/raw_videos \
  --output data/landmarks
```

### ğŸ§  Train hybrid model
```bash
python src/training/train_hybrid.py \
  --config configs/train_hybrid_len64.json
```

### ğŸ” Run inference
```bash
python src/inference/run_inference.py \
  --video Flapping_her_arms_KPuLA5LlVjg.mp4 \
  --model runs_dl/hybrid_len64_clean_finetune \
  --output figures/Figure_4_4_alignment.png
```

---

## ğŸ“ˆ Visual Examples

- **Pipeline Diagram:**  
  ![Pipeline](figures/pipeline.png)

- **Prediction Alignment (Fig. 4.4):**  
  ![Predictions](figures/Figure_4_4_alignment.png)

---

## ğŸ§­ Citation

If you use this code, please cite:

> Chen, S. (2025). *Temporal Modelling and Classification of Hand Flapping Form in Autism Spectrum Disorder.*  
> Masterâ€™s Dissertation, University of Malaya.

and prior work:

> Mohammed, H. A. (2023). *Hand Flapping Detection Using Pose Estimation.*  
> University of Malaya.

---

## ğŸª„ Acknowledgements

This project extends the baseline framework of **Mohammed (2023)** and was supervised by **Dr. Zati Hakim Azizul Hasan**  
at the Department of Artificial Intelligence, Faculty of Computer Science and Information Technology, Universiti Malaya.

---

## ğŸ§© License

This repository is distributed under the **MIT License**.  
Raw video data from the **ASBD dataset** is not redistributed here due to privacy and licensing constraints.  
Processed features and trained models are available upon reasonable request.

---

## ğŸŒ Keywords

`Autism Spectrum Disorder` â€¢ `Hand Flapping` â€¢ `Pose Estimation` â€¢ `Deep Learning` â€¢ `CNNâ€“LSTM` â€¢ `Temporal Modelling` â€¢ `Computer Vision`
